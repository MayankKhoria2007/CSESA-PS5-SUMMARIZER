{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayankKhoria2007/CSESA-PS5-SUMMARIZER/blob/main/SUMMARIZERT5HYBRIDWITHTEXTRANK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV_s2gyk4sWa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"]=\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyGbMRBlj535"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxr3FnKPkdCG"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvmdpeRNqPe-"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "ds=load_dataset(\"NortheasternUniversity/big_patent\",\"a\",split=\"train[:6%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWmW-tqyuZSv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_name=\"t5-small\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikTDMlAyu3Um"
      },
      "outputs": [],
      "source": [
        "prefix=\"summarize\"\n",
        "def preprocess(examples):\n",
        "  input=[prefix+ i for i in examples[\"description\"]]\n",
        "\n",
        "  model_inputs=tokenizer(input,max_length=512,truncation=True,padding=\"max_length\")\n",
        "  labels=tokenizer(text_target=examples[\"abstract\"],max_length=128,truncation=True,padding=\"max_length\")\n",
        "  model_inputs[\"labels\"]=labels[\"input_ids\"]\n",
        "  return model_inputs\n",
        "raw_dataset=ds.train_test_split(test_size=0.2)\n",
        "eval_dataset=raw_dataset[\"test\"]\n",
        "toeknized_ds=ds.map(preprocess,batched=True,remove_columns=ds.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxj0RGa3rQQB"
      },
      "outputs": [],
      "source": [
        "\n",
        "toeknized_ds=toeknized_ds.train_test_split(test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUDadU3mriwI"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMGxD7o1VHgk"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "rouge=evaluate.load(\"rouge\")\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions,labels=eval_pred\n",
        "  decoded_preds=tokenizer.batch_decode(predictions,skip_special_tokens=True)\n",
        "  labels=np.where(labels!=-100,labels,tokenizer.pad_token_id)\n",
        "  decoded_labels=tokenizer.batch_decode(labels,skip_special_tokens=True)\n",
        "  result=rouge.compute(predictions=decoded_preds,references=decoded_labels,use_stemmer=True)\n",
        "  prediction_lens=[np.count_nonzero(pred!=tokenizer.pad_token_id) for pred in predictions]\n",
        "  result[\"gen_len\"]=np.mean(prediction_lens)\n",
        "  return {k:round(v,4) for k,v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbDahuqR-XJN"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model_name,padding=\"longest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWitqaIB_H4m"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "model=AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "736nmAOPA1Hu"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./t5-bigpatent\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=False,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    fp16=False)\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=toeknized_ds[\"train\"],\n",
        "    eval_dataset=toeknized_ds[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=None,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score\n",
        "bert=evaluate.load(\"bertscore\")"
      ],
      "metadata": {
        "id": "6wTpjrxmLxT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCkGceahk1WP"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./t5_bigpatent_model\")\n",
        "tokenizer.save_pretrained(\"./t5_bigpatent_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgi6NTFznzRK"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    words = [\n",
        "        lemmatizer.lemmatize(w)\n",
        "        for w in words\n",
        "        if w.isalpha() and w not in stop_words\n",
        "    ]\n",
        "    return \" \".join(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-EdtMK9n5DI"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def textrank_extractive_lemmatized(text, k=7):\n",
        "    sentences = sent_tokenize(text)\n",
        "    processed = [lemmatize_sentence(s) for s in sentences]\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    tfidf = vectorizer.fit_transform(processed)\n",
        "    similarity_matrix = cosine_similarity(tfidf)\n",
        "    graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(graph)\n",
        "    ranked_sentences = sorted(\n",
        "        ((scores[i], s, i) for i, s in enumerate(sentences)),\n",
        "        reverse=True\n",
        "    )\n",
        "    selected = sorted(ranked_sentences[:k], key=lambda x: x[2])\n",
        "    extracted_text = \" \".join([s[1] for s in selected])\n",
        "    return extracted_text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnrI4MCHm15v"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"./t5_bigpatent_model\",   # your trained model path\n",
        "    tokenizer=\"./t5_bigpatent_model\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "def hybrid_summarizer_pipeline(text):\n",
        "    # Extractive phase\n",
        "    extracted_text = textrank_extractive_lemmatized(text, k=7)\n",
        "    extracted_text=extracted_text[:1500]\n",
        "    torch.cuda.empty_cache()\n",
        "    # Abstractive phase using pipeline\n",
        "    with torch.no_grad():\n",
        "      summary = summarizer(\n",
        "          \"summarize:\"+extracted_text,\n",
        "          max_length=150,\n",
        "          min_length=40,\n",
        "          num_beams=2,\n",
        "          do_sample=False\n",
        "      )\n",
        "    torch.cuda.empty_cache()\n",
        "    return summary[0][\"summary_text\"]\n",
        "\n",
        "\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "c=0\n",
        "\n",
        "for ex in eval_dataset.select(range(200)):\n",
        "    text = ex[\"description\"]\n",
        "    ref = ex[\"abstract\"]\n",
        "\n",
        "    pred = hybrid_summarizer_pipeline(text)\n",
        "\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(ref)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P,R,F1 = bert.compute(predictions=predictions, references=references,lang='en')\n",
        "print(f\"BERTScore Precision:{P.mean().item():.4f}\")\n",
        "print(f\"BERTScore Recall:{R.mean().item():.4f}\")\n",
        "print(f\"BERTScore F1:{F1.mean().item():.4f}\")\n"
      ],
      "metadata": {
        "id": "5mB3WKbVOXHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=\"Artificial intelligence systems are increasingly being integrated into modern technological solutions across multiple industries. These systems rely on advanced machine learning algorithms that enable computers to learn patterns from large volumes of data and make intelligent decisions. In recent years, deep learning techniques based on neural networks have shown significant improvements in tasks such as image recognition, natural language processing, and speech analysis. However, training deep learning models requires substantial computational resources, including high-performance processors and graphical processing units. The growing size of datasets and model parameters has made scalability a major challenge. To address this issue, researchers have proposed optimization techniques such as model compression, distributed training, and hybrid processing pipelines. Hybrid systems combine multiple approaches to improve efficiency and performance. For example, in text summarization, extractive methods can be used to identify the most relevant portions of a document, while abstractive models generate concise and fluent summaries. This combination reduces input length while preserving essential information. Such hybrid approaches are particularly useful for processing long technical documents, including patents and research articles. By integrating statistical methods with neural models, hybrid systems achieve better accuracy, faster processing, and improved adaptability to real-world applications.\"\n",
        "print(hybrid_summarizer_pipeline(t))"
      ],
      "metadata": {
        "id": "YRXE8wTXS-mq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPdqpTrt141shiIKY1Sc5G8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}